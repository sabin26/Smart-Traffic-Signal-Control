import marimo

__generated_with = "0.18.4"
app = marimo.App(width="medium")


@app.cell(hide_code=True)
def title_cell():
    import marimo as mo

    mo.md(
        """
        # ðŸš¦ Smart Traffic Signal Control in Kathmandu
        ## Using Proximal Policy Optimization (PPO)

        **Student:** Sabin Neupane | **ID:** 250136 | **Module:** Artificial Neural Network (STW7088CEM)

        This notebook implements an adaptive traffic signal control system using:
        - **Baseline:** Fixed-time controller and supervised MLP
        - **RL Agent:** Proximal Policy Optimization (PPO) with Actor-Critic architecture

        ---
        """
    )
    return (mo,)


@app.cell(hide_code=True)
def imports_cell(mo):
    mo.md("""
    ## 1. Setup and Imports
    """)
    return


@app.cell
def setup_imports():
    import numpy as np
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from torch.distributions import Categorical
    import matplotlib.pyplot as plt
    from collections import deque, namedtuple
    import random
    from dataclasses import dataclass
    from typing import List, Tuple, Optional
    import warnings

    warnings.filterwarnings("ignore")

    # Set seeds for reproducibility
    SEED = 42
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    random.seed(SEED)

    # Device configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    return dataclass, device, nn, np, optim, plt, torch


@app.cell(hide_code=True)
def config_header(mo):
    mo.md("""
    ## 2. Configuration and Hyperparameters
    """)
    return


@app.cell
def configuration(dataclass):
    @dataclass
    class Config:
        """Configuration for the Smart Traffic Signal Control System"""

        # Environment settings
        num_lanes: int = 4  # Number of approach lanes (N, S, E, W)
        max_vehicles_per_lane: int = 20  # Max queue capacity per lane
        max_steps_per_episode: int = 300  # Steps per episode
        green_duration: int = 30  # Default green phase duration (seconds)
        yellow_duration: int = 5  # Yellow phase duration (seconds)
        min_green: int = 10  # Minimum green phase duration
        max_green: int = 60  # Maximum green phase duration

        # State and action space
        state_dim: int = 12  # Queue lengths (4) + waiting times (4) + current phase (4)
        action_dim: int = 4  # 4 possible signal phases

        # PPO Hyperparameters (simplified)
        gamma: float = 0.95  # Lower discount factor (focus on immediate queue clearing)
        gae_lambda: float = 0.95  # GAE lambda
        clip_epsilon: float = 0.2  # PPO clipping
        entropy_coef: float = 0.01  # Exploration
        value_coef: float = 0.5  # Value loss coefficient
        max_grad_norm: float = 0.5  # Gradient clipping
        learning_rate: float = 1e-3  # Standard learning rate
        batch_size: int = 64  # Standard batch size
        n_epochs: int = 4  # Standard epochs
        update_interval: int = 512  # More collected data before update

        # Training settings
        num_episodes: int = 400  # Training episodes
        eval_interval: int = 10  # Evaluation frequency

        # MLP Baseline settings
        mlp_hidden_dim: int = 128
        mlp_epochs: int = 100
        mlp_lr: float = 1e-3

    config = Config()
    print("Configuration loaded:")
    print(f"  - State dimension: {config.state_dim}")
    print(f"  - Action dimension: {config.action_dim}")
    print(f"  - PPO clip epsilon: {config.clip_epsilon}")
    print(f"  - Learning rate: {config.learning_rate}")
    print(f"  - Update interval: {config.update_interval}")
    print(f"  - Training episodes: {config.num_episodes}")
    return (config,)


@app.cell(hide_code=True)
def env_header(mo):
    mo.md("""
    ## 3. Traffic Environment Simulation

    Since SUMO requires external installation, we implement a **realistic traffic simulation**
    that models Kathmandu's traffic patterns including:
    - Variable traffic flow based on time of day (rush hours)
    - Random vehicle arrivals following Poisson distribution
    - Queue dynamics and waiting time accumulation
    """)
    return


@app.cell
def traffic_environment(config, np):
    class TrafficIntersection:
        """
        Simulated 4-way intersection for Kathmandu traffic.

        State Space:
        - Queue lengths for each lane (4 values)
        - Average waiting time per lane (4 values)
        - One-hot encoded current phase (4 values)

        Action Space:
        - 0: North-South green
        - 1: East-West green
        - 2: North-South left turn
        - 3: East-West left turn
        """

        def __init__(self, config):
            self.config = config
            self.num_lanes = config.num_lanes
            self.max_vehicles = config.max_vehicles_per_lane
            self.max_steps = config.max_steps_per_episode

            # Traffic flow rates (vehicles per step) - varies by time of day
            self.base_arrival_rates = np.array([0.3, 0.3, 0.4, 0.4])  # N, S, E, W

            # Initialize state
            self.reset()

        def reset(self):
            """Reset the environment to initial state"""
            self.step_count = 0
            self.current_phase = 0
            self.phase_timer = 0

            # Queue lengths per lane
            self.queue_lengths = np.random.randint(0, 5, size=self.num_lanes).astype(
                float
            )

            # Waiting time accumulator per vehicle
            self.waiting_times = np.zeros(self.num_lanes)

            # Total vehicles that passed through
            self.total_throughput = 0
            self.total_waiting_time = 0

            # Time of day (0-24 hours, affects traffic density)
            self.time_of_day = np.random.uniform(6, 22)  # Operating hours

            return self._get_state()

        def _get_arrival_rate(self):
            """Get traffic arrival rate based on time of day (Kathmandu patterns)"""
            hour = self.time_of_day

            # Rush hour multipliers (morning 8-10, evening 5-7)
            if 8 <= hour <= 10:
                multiplier = 2.0  # Morning rush
            elif 17 <= hour <= 19:
                multiplier = 2.5  # Evening rush (heavier in Kathmandu)
            elif 12 <= hour <= 14:
                multiplier = 1.3  # Lunch time
            elif 22 <= hour or hour <= 6:
                multiplier = 0.3  # Night time
            else:
                multiplier = 1.0  # Normal

            # Add some randomness
            noise = np.random.uniform(0.8, 1.2, size=self.num_lanes)
            return self.base_arrival_rates * multiplier * noise

        def _get_state(self):
            """Construct the state vector"""
            # Normalize queue lengths
            norm_queues = self.queue_lengths / self.max_vehicles

            # Normalize waiting times (assume max 120 seconds)
            norm_waiting = np.clip(self.waiting_times / 120.0, 0, 1)

            # One-hot encode current phase
            phase_one_hot = np.zeros(self.num_lanes)
            phase_one_hot[self.current_phase] = 1.0

            state = np.concatenate([norm_queues, norm_waiting, phase_one_hot])
            return state.astype(np.float32)

        def _get_green_lanes(self, action):
            """Get which lanes have green light based on action"""
            if action == 0:  # North-South through
                return [0, 1]
            elif action == 1:  # East-West through
                return [2, 3]
            elif action == 2:  # North-South left turn
                return [0]
            else:  # East-West left turn
                return [2]

        def step(self, action):
            """Execute one step in the environment"""
            self.step_count += 1

            # Phase change penalty (yellow light delay)
            phase_change_penalty = 0
            if action != self.current_phase:
                phase_change_penalty = self.config.yellow_duration * 0.1
                self.current_phase = action

            # Get lanes with green light
            green_lanes = self._get_green_lanes(action)

            # Process vehicle departures (green lanes)
            departures = np.zeros(self.num_lanes)
            for lane in green_lanes:
                # Saturation flow rate: ~0.5 vehicles per second equivalent
                max_depart = min(2, self.queue_lengths[lane])
                departures[lane] = max_depart
                self.queue_lengths[lane] -= departures[lane]
                self.total_throughput += departures[lane]

            # Process vehicle arrivals (Poisson process)
            arrival_rates = self._get_arrival_rate()
            arrivals = np.random.poisson(arrival_rates)
            self.queue_lengths = np.clip(
                self.queue_lengths + arrivals, 0, self.max_vehicles
            )

            # Update waiting times
            for i in range(self.num_lanes):
                if i in green_lanes:
                    # Reduce waiting time for departing vehicles
                    self.waiting_times[i] = max(
                        0, self.waiting_times[i] - departures[i] * 5
                    )
                else:
                    # Increase waiting time for queued vehicles
                    self.waiting_times[i] += self.queue_lengths[i] * 1.0

            self.total_waiting_time += np.sum(self.waiting_times)

            # Advance time of day
            self.time_of_day = (self.time_of_day + 1 / 60) % 24  # 1 minute per step

            # Simplified Reward: Direct pressure minimization (like Max Pressure)
            # Max Pressure action selection rule: argmax(pressure). Therefore, R = -pressure
            
            # Pressure = sum of queue lengths on incoming lanes
            pressure = np.sum(self.queue_lengths)
            
            # Base reward is simply negative pressure (minimize queues)
            reward = -pressure * 0.1
            
            # Add throughput reward to encourage moving vehicles
            reward += np.sum(departures) * 1.0
            
            # Small penalty for phase changes to prevent flickering
            reward -= phase_change_penalty * 0.2

            # Check if episode is done
            done = self.step_count >= self.max_steps

            # Check if episode is done
            done = self.step_count >= self.max_steps

            # Get next state
            next_state = self._get_state()

            info = {
                "queue_lengths": self.queue_lengths.copy(),
                "waiting_times": self.waiting_times.copy(),
                "throughput": self.total_throughput,
                "arrivals": arrivals,
                "departures": departures,
            }

            return next_state, reward, done, info

        def get_metrics(self):
            """Get performance metrics"""
            return {
                "avg_queue_length": np.mean(self.queue_lengths),
                "total_queue_length": np.sum(self.queue_lengths),
                "avg_waiting_time": np.mean(self.waiting_times),
                "total_waiting_time": self.total_waiting_time,
                "throughput": self.total_throughput,
            }

    # Test the environment
    env = TrafficIntersection(config)
    state = env.reset()
    print(f"Initial state shape: {state.shape}")
    print(f"Initial state: {state}")

    # Take a random action
    next_state, reward, done, info = env.step(np.random.randint(0, 4))
    print(f"\nAfter random action:")
    print(f"  Reward: {reward:.4f}")
    print(f"  Queue lengths: {info['queue_lengths']}")
    return (TrafficIntersection,)


@app.cell(hide_code=True)
def baseline_header(mo):
    mo.md("""
    ## 4. Baseline Controllers

    ### 4.1 Fixed-Time Controller
    Traditional signal controller with fixed green phase durations.

    ### 4.2 MLP Baseline (Supervised Learning)
    Multilayer Perceptron trained to predict optimal actions from historical data.
    """)
    return


@app.cell
def fixed_time_controller(np):
    class FixedTimeController:
        """
        Traditional fixed-time traffic signal controller.
        Cycles through phases with fixed durations.
        """

        def __init__(self, phase_duration=30, num_phases=4):
            self.phase_duration = phase_duration
            self.num_phases = num_phases
            self.current_phase = 0
            self.timer = 0

        def reset(self):
            self.current_phase = 0
            self.timer = 0

        def get_action(self, state=None):
            """Get action based on fixed timing (ignores state)"""
            self.timer += 1
            if self.timer >= self.phase_duration:
                self.timer = 0
                self.current_phase = (self.current_phase + 1) % self.num_phases
            return self.current_phase

    class MaxPressureController:
        """
        Max-Pressure adaptive controller.
        Selects the phase that relieves maximum queue pressure.
        """

        def __init__(self):
            pass

        def get_action(self, state):
            """Select action based on maximum queue pressure"""
            # Extract queue lengths from state (first 4 values)
            queue_lengths = state[:4] * 20  # Denormalize

            # Calculate pressure for each phase
            pressures = np.array(
                [
                    queue_lengths[0] + queue_lengths[1],  # N-S through
                    queue_lengths[2] + queue_lengths[3],  # E-W through
                    queue_lengths[0],  # N-S left
                    queue_lengths[2],  # E-W left
                ]
            )

            return np.argmax(pressures)

    print("Baseline controllers defined:")
    print("  - FixedTimeController: Cycles through phases with fixed duration")
    print("  - MaxPressureController: Selects phase with maximum queue pressure")
    return FixedTimeController, MaxPressureController


@app.cell
def mlp_baseline(config, device, nn, np, torch):
    class MLPBaseline(nn.Module):
        """
        Supervised MLP for traffic signal control.
        Trained on historical data to predict queue lengths and optimal actions.
        """

        def __init__(self, input_dim, hidden_dim, output_dim):
            super(MLPBaseline, self).__init__()

            self.network = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(0.2),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(0.2),
                nn.Linear(hidden_dim, output_dim),
            )

        def forward(self, x):
            return self.network(x)

    class MLPActionPredictor(nn.Module):
        """
        MLP that predicts optimal action given current state.
        Uses softmax output for action probabilities.
        """

        def __init__(self, state_dim, hidden_dim, action_dim):
            super(MLPActionPredictor, self).__init__()

            self.network = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Linear(hidden_dim // 2, action_dim),
            )

        def forward(self, x):
            logits = self.network(x)
            return logits

        def get_action(self, state):
            """Get action from state"""
            if isinstance(state, np.ndarray):
                state = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                logits = self.forward(state)
                action = torch.argmax(logits, dim=-1)
            return action.item()

    # Initialize MLP baseline
    mlp_predictor = MLPActionPredictor(
        state_dim=config.state_dim,
        hidden_dim=config.mlp_hidden_dim,
        action_dim=config.action_dim,
    ).to(device)

    print(f"MLP Action Predictor architecture:")
    print(mlp_predictor)
    print(f"\nTotal parameters: {sum(p.numel() for p in mlp_predictor.parameters()):,}")
    return (mlp_predictor,)


@app.cell(hide_code=True)
def ppo_header(mo):
    mo.md("""
    ## 5. PPO Implementation

    ### Actor-Critic Architecture
    - **Actor Network**: Outputs action probabilities (policy Ï€(a|s))
    - **Critic Network**: Estimates state value V(s)

    ### PPO Algorithm Features
    - Clipped surrogate objective for stable updates
    - Generalized Advantage Estimation (GAE)
    - Entropy bonus for exploration
    """)
    return


@app.cell
def actor_critic_networks(config, device, nn, np, torch):
    class ActorNetwork(nn.Module):
        """
        Actor network for PPO.
        Outputs action probabilities using softmax.
        """

        def __init__(self, state_dim, hidden_dim, action_dim):
            super(ActorNetwork, self).__init__()

            self.network = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.Tanh(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.Tanh(),
                nn.Linear(hidden_dim, action_dim),
            )

            # Initialize weights
            self._init_weights()

        def _init_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
                    nn.init.constant_(m.bias, 0.0)

        def forward(self, state):
            logits = self.network(state)
            return logits

        def get_action_probs(self, state):
            logits = self.forward(state)
            probs = torch.softmax(logits, dim=-1)
            return probs

    class CriticNetwork(nn.Module):
        """
        Critic network for PPO.
        Estimates state value function V(s).
        """

        def __init__(self, state_dim, hidden_dim):
            super(CriticNetwork, self).__init__()

            self.network = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.Tanh(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.Tanh(),
                nn.Linear(hidden_dim, 1),
            )

            # Initialize weights
            self._init_weights()

        def _init_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
                    nn.init.constant_(m.bias, 0.0)

        def forward(self, state):
            value = self.network(state)
            return value

    class ActorCritic(nn.Module):
        """
        Combined Actor-Critic network for PPO.
        """

        def __init__(self, state_dim, hidden_dim, action_dim):
            super(ActorCritic, self).__init__()

            # Shared feature extractor (simple 2-layer)
            self.shared = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.Tanh(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.Tanh(),
            )

            # Actor head
            self.actor = nn.Linear(hidden_dim, action_dim)

            # Critic head
            self.critic = nn.Linear(hidden_dim, 1)

            self._init_weights()

        def _init_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
                    nn.init.constant_(m.bias, 0.0)

        def forward(self, state):
            features = self.shared(state)
            action_logits = self.actor(features)
            value = self.critic(features)
            return action_logits, value

        def get_action(self, state, deterministic=False):
            """Sample action from policy"""
            if isinstance(state, np.ndarray):
                state = torch.FloatTensor(state).unsqueeze(0).to(device)

            action_logits, value = self.forward(state)
            probs = torch.softmax(action_logits, dim=-1)
            dist = torch.distributions.Categorical(probs)

            if deterministic:
                action = torch.argmax(probs, dim=-1)
            else:
                action = dist.sample()

            log_prob = dist.log_prob(action)

            return action.item(), log_prob.item(), value.item()

        def evaluate_actions(self, states, actions):
            """Evaluate actions for PPO update"""
            action_logits, values = self.forward(states)
            probs = torch.softmax(action_logits, dim=-1)
            dist = torch.distributions.Categorical(probs)

            log_probs = dist.log_prob(actions)
            entropy = dist.entropy()

            return log_probs, values.squeeze(-1), entropy

    # Initialize Actor-Critic network
    actor_critic = ActorCritic(
        state_dim=config.state_dim, hidden_dim=256, action_dim=config.action_dim
    ).to(device)

    print("Actor-Critic Network Architecture:")
    print(actor_critic)
    print(f"\nTotal parameters: {sum(p.numel() for p in actor_critic.parameters()):,}")
    return (ActorCritic,)


@app.cell
def ppo_agent(device, np, optim, torch):
    class PPOMemory:
        """Experience buffer for PPO"""

        def __init__(self):
            self.states = []
            self.actions = []
            self.rewards = []
            self.values = []
            self.log_probs = []
            self.dones = []

        def store(self, state, action, reward, value, log_prob, done):
            self.states.append(state)
            self.actions.append(action)
            self.rewards.append(reward)
            self.values.append(value)
            self.log_probs.append(log_prob)
            self.dones.append(done)

        def clear(self):
            self.states.clear()
            self.actions.clear()
            self.rewards.clear()
            self.values.clear()
            self.log_probs.clear()
            self.dones.clear()

        def get_batches(self, batch_size):
            n_samples = len(self.states)
            indices = np.arange(n_samples)
            np.random.shuffle(indices)

            for start in range(0, n_samples, batch_size):
                end = start + batch_size
                batch_indices = indices[start:end]
                yield batch_indices

    class PPOAgent:
        """
        Proximal Policy Optimization Agent for Traffic Signal Control.
        """

        def __init__(self, actor_critic, config):
            self.actor_critic = actor_critic
            self.config = config

            self.optimizer = optim.Adam(
                actor_critic.parameters(), lr=config.learning_rate, eps=1e-5
            )

            self.memory = PPOMemory()

            # Training statistics
            self.policy_losses = []
            self.value_losses = []
            self.entropy_losses = []

        def select_action(self, state, deterministic=False):
            """Select action using current policy"""
            return self.actor_critic.get_action(state, deterministic)

        def store_transition(self, state, action, reward, value, log_prob, done):
            """Store transition in memory"""
            self.memory.store(state, action, reward, value, log_prob, done)

        def compute_gae(self, rewards, values, dones, next_value):
            """Compute Generalized Advantage Estimation"""
            advantages = []
            gae = 0

            # Add next value for bootstrapping
            values = values + [next_value]

            for t in reversed(range(len(rewards))):
                if dones[t]:
                    delta = rewards[t] - values[t]
                    gae = delta
                else:
                    delta = (
                        rewards[t] + self.config.gamma * values[t + 1] - values[t]
                    )
                    gae = delta + self.config.gamma * self.config.gae_lambda * gae

                advantages.insert(0, gae)

            advantages = np.array(advantages)
            returns = advantages + np.array(values[:-1])

            return advantages, returns

        def update(self, next_value):
            """Perform PPO update"""
            # Get data from memory
            states = np.array(self.memory.states)
            actions = np.array(self.memory.actions)
            old_log_probs = np.array(self.memory.log_probs)
            rewards = self.memory.rewards
            values = self.memory.values
            dones = self.memory.dones

            # Compute advantages and returns
            advantages, returns = self.compute_gae(rewards, values, dones, next_value)

            # Normalize advantages
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

            # Convert to tensors
            states = torch.FloatTensor(states).to(device)
            actions = torch.LongTensor(actions).to(device)
            old_log_probs = torch.FloatTensor(old_log_probs).to(device)
            advantages = torch.FloatTensor(advantages).to(device)
            returns = torch.FloatTensor(returns).to(device)

            # PPO update for multiple epochs
            total_policy_loss = 0
            total_value_loss = 0
            total_entropy_loss = 0
            n_updates = 0

            for _ in range(self.config.n_epochs):
                for batch_indices in self.memory.get_batches(self.config.batch_size):
                    batch_states = states[batch_indices]
                    batch_actions = actions[batch_indices]
                    batch_old_log_probs = old_log_probs[batch_indices]
                    batch_advantages = advantages[batch_indices]
                    batch_returns = returns[batch_indices]

                    # Evaluate actions
                    new_log_probs, values_pred, entropy = (
                        self.actor_critic.evaluate_actions(batch_states, batch_actions)
                    )

                    # Policy loss with clipping
                    ratio = torch.exp(new_log_probs - batch_old_log_probs)
                    surr1 = ratio * batch_advantages
                    surr2 = (
                        torch.clamp(
                            ratio,
                            1 - self.config.clip_epsilon,
                            1 + self.config.clip_epsilon,
                        )
                        * batch_advantages
                    )
                    policy_loss = -torch.min(surr1, surr2).mean()

                    # Value loss
                    value_loss = (
                        self.config.value_coef
                        * (batch_returns - values_pred).pow(2).mean()
                    )

                    # Entropy loss (negative because we want to maximize entropy)
                    entropy_loss = -self.config.entropy_coef * entropy.mean()

                    # Total loss
                    loss = policy_loss + value_loss + entropy_loss

                    # Optimize
                    self.optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(
                        self.actor_critic.parameters(), self.config.max_grad_norm
                    )
                    self.optimizer.step()

                    total_policy_loss += policy_loss.item()
                    total_value_loss += value_loss.item()
                    total_entropy_loss += entropy_loss.item()
                    n_updates += 1

            # Store average losses
            self.policy_losses.append(total_policy_loss / n_updates)
            self.value_losses.append(total_value_loss / n_updates)
            self.entropy_losses.append(total_entropy_loss / n_updates)

            # Clear memory
            self.memory.clear()

            return {
                "policy_loss": total_policy_loss / n_updates,
                "value_loss": total_value_loss / n_updates,
                "entropy_loss": total_entropy_loss / n_updates,
            }

    print("PPO Agent components defined:")
    print("  - PPOMemory: Experience buffer with GAE support")
    print("  - PPOAgent: Full PPO implementation with clipped objective")
    return (PPOAgent,)


@app.cell(hide_code=True)
def training_header(mo):
    mo.md("""
    ## 6. Training Functions

    Training loop for:
    1. Collecting experience data from environment
    2. Updating PPO agent
    3. Evaluating performance
    """)
    return


@app.cell
def training_functions(device, np, torch):
    def train_ppo(agent, env, config, verbose=True):
        """Train PPO agent on traffic environment with improved training loop"""
        episode_rewards = []
        episode_lengths = []
        avg_queue_lengths = []
        avg_waiting_times = []

        total_steps = 0
        best_reward = float('-inf')
        
        # Learning rate scheduler
        initial_lr = config.learning_rate
        
        for episode in range(config.num_episodes):
            state = env.reset()
            episode_reward = 0
            episode_length = 0
            
            # Learning rate decay
            lr_multiplier = max(0.1, 1.0 - episode / config.num_episodes)
            for param_group in agent.optimizer.param_groups:
                param_group['lr'] = initial_lr * lr_multiplier

            for step in range(config.max_steps_per_episode):
                # Select action
                action, log_prob, value = agent.select_action(state)

                # Take step in environment
                next_state, reward, done, info = env.step(action)

                # Store transition
                agent.store_transition(state, action, reward, value, log_prob, done)

                episode_reward += reward
                episode_length += 1
                total_steps += 1

                state = next_state

                # Update agent periodically
                if total_steps % config.update_interval == 0 and len(agent.memory.states) >= config.batch_size:
                    # Get next value for bootstrapping
                    with torch.no_grad():
                        state_tensor = (
                            torch.FloatTensor(state).unsqueeze(0).to(device)
                        )
                        _, next_value = agent.actor_critic(state_tensor)
                        next_value = next_value.item()

                    agent.update(next_value)

                if done:
                    break
            
            # Force update at end of episode if we have enough data
            if len(agent.memory.states) >= config.batch_size:
                with torch.no_grad():
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                    _, next_value = agent.actor_critic(state_tensor)
                    next_value = next_value.item() if not done else 0
                agent.update(next_value)

            # Get final metrics
            metrics = env.get_metrics()
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            avg_queue_lengths.append(metrics["avg_queue_length"])
            avg_waiting_times.append(metrics["avg_waiting_time"])
            
            # Track best performance
            if episode_reward > best_reward:
                best_reward = episode_reward

            if verbose and (episode + 1) % 10 == 0:
                recent_rewards = episode_rewards[-10:]
                print(
                    f"Episode {episode + 1}/{config.num_episodes} | "
                    f"Reward: {episode_reward:.2f} | "
                    f"Avg(10): {np.mean(recent_rewards):.2f} | "
                    f"Best: {best_reward:.2f} | "
                    f"Queue: {metrics['avg_queue_length']:.2f} | "
                    f"LR: {lr_multiplier * initial_lr:.6f}"
                )

        return {
            "episode_rewards": episode_rewards,
            "episode_lengths": episode_lengths,
            "avg_queue_lengths": avg_queue_lengths,
            "avg_waiting_times": avg_waiting_times,
            "policy_losses": agent.policy_losses,
            "value_losses": agent.value_losses,
        }

    def evaluate_controller(controller, env, num_episodes=10, is_ppo=False):
        """Evaluate a controller's performance"""
        total_rewards = []
        total_queue_lengths = []
        total_waiting_times = []
        total_throughputs = []

        for _ in range(num_episodes):
            state = env.reset()
            if hasattr(controller, "reset"):
                controller.reset()

            episode_reward = 0

            for _ in range(env.max_steps):
                if is_ppo:
                    action, _, _ = controller.select_action(state, deterministic=True)
                elif hasattr(controller, "get_action"):
                    if isinstance(controller, type) or callable(
                        getattr(controller, "get_action", None)
                    ):
                        action = controller.get_action(state)
                    else:
                        action = controller.get_action(state)
                else:
                    action = np.random.randint(0, 4)

                next_state, reward, done, _ = env.step(action)
                episode_reward += reward
                state = next_state

                if done:
                    break

            metrics = env.get_metrics()
            total_rewards.append(episode_reward)
            total_queue_lengths.append(metrics["avg_queue_length"])
            total_waiting_times.append(metrics["avg_waiting_time"])
            total_throughputs.append(metrics["throughput"])

        return {
            "mean_reward": np.mean(total_rewards),
            "std_reward": np.std(total_rewards),
            "mean_queue_length": np.mean(total_queue_lengths),
            "mean_waiting_time": np.mean(total_waiting_times),
            "mean_throughput": np.mean(total_throughputs),
        }

    print("Training functions defined:")
    print("  - train_ppo(): Main training loop")
    print("  - evaluate_controller(): Evaluation function for any controller")
    return evaluate_controller, train_ppo


@app.cell(hide_code=True)
def run_training_header(mo):
    mo.md("""
    ## 7. Run Training

    Execute training for:
    1. PPO Agent (main RL approach)
    2. Evaluate against baselines
    """)
    return


@app.cell
def create_mlp_training_data(
    MaxPressureController,
    TrafficIntersection,
    config,
    np,
):
    def generate_training_data(env, num_episodes=50):
        """Generate training data using Max Pressure controller for MLP baseline"""
        states = []
        actions = []

        controller = MaxPressureController()

        for _ in range(num_episodes):
            state = env.reset()

            for _ in range(env.max_steps):
                action = controller.get_action(state)

                states.append(state)
                actions.append(action)

                next_state, _, done, _ = env.step(action)
                state = next_state

                if done:
                    break

        return np.array(states), np.array(actions)

    # Generate data for MLP training
    data_env = TrafficIntersection(config)
    mlp_states, mlp_actions = generate_training_data(data_env, num_episodes=30)
    print(f"Generated {len(mlp_states)} training samples for MLP baseline")
    print(f"States shape: {mlp_states.shape}, Actions shape: {mlp_actions.shape}")
    return mlp_actions, mlp_states


@app.cell
def train_mlp_baseline(
    config,
    device,
    mlp_actions,
    mlp_predictor,
    mlp_states,
    nn,
    np,
    optim,
    torch,
):
    # Train MLP baseline
    def train_mlp(model, states, actions, epochs=100, lr=1e-3, batch_size=64):
        """Train MLP on supervised data"""
        optimizer = optim.Adam(model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss()

        # Convert to tensors
        states_tensor = torch.FloatTensor(states).to(device)
        actions_tensor = torch.LongTensor(actions).to(device)

        n_samples = len(states)
        losses = []

        model.train()
        for epoch in range(epochs):
            # Shuffle data
            indices = np.random.permutation(n_samples)
            epoch_loss = 0
            n_batches = 0

            for start in range(0, n_samples, batch_size):
                end = min(start + batch_size, n_samples)
                batch_idx = indices[start:end]

                batch_states = states_tensor[batch_idx]
                batch_actions = actions_tensor[batch_idx]

                optimizer.zero_grad()
                logits = model(batch_states)
                loss = criterion(logits, batch_actions)
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()
                n_batches += 1

            avg_loss = epoch_loss / n_batches
            losses.append(avg_loss)

            if (epoch + 1) % 20 == 0:
                print(f"MLP Epoch {epoch + 1}/{epochs} | Loss: {avg_loss:.4f}")

        return losses

    print("Training MLP Baseline...")
    mlp_losses = train_mlp(
        mlp_predictor,
        mlp_states,
        mlp_actions,
        epochs=config.mlp_epochs,
        lr=config.mlp_lr,
    )
    print("MLP training complete!")
    return (mlp_losses,)


@app.cell
def run_ppo_training(
    ActorCritic,
    PPOAgent,
    TrafficIntersection,
    config,
    device,
    train_ppo,
):
    # Initialize fresh environment and agent for training
    print("=" * 60)
    print("Starting PPO Training for Smart Traffic Signal Control")
    print("=" * 60)

    training_env = TrafficIntersection(config)

    # Create new actor-critic for training
    training_actor_critic = ActorCritic(
        state_dim=config.state_dim, hidden_dim=256, action_dim=config.action_dim
    ).to(device)

    ppo_agent = PPOAgent(training_actor_critic, config)

    # Train the agent
    training_results = train_ppo(ppo_agent, training_env, config, verbose=True)

    print("\n" + "=" * 60)
    print("PPO Training Complete!")
    print("=" * 60)
    return ppo_agent, training_results


@app.cell(hide_code=True)
def evaluation_header(mo):
    mo.md("""
    ## 8. Evaluation and Comparison

    Compare the performance of:
    - **Fixed-Time Controller** (baseline)
    - **Max Pressure Controller** (adaptive baseline)
    - **MLP Predictor** (supervised learning)
    - **PPO Agent** (reinforcement learning)
    """)
    return


@app.cell
def run_evaluation(
    FixedTimeController,
    MaxPressureController,
    TrafficIntersection,
    config,
    evaluate_controller,
    mlp_predictor,
    ppo_agent,
):
    print("Evaluating all controllers...")
    print("-" * 60)

    eval_env = TrafficIntersection(config)

    # Evaluate Fixed-Time Controller
    fixed_controller = FixedTimeController(phase_duration=30)
    fixed_results = evaluate_controller(fixed_controller, eval_env, num_episodes=20)
    print(f"Fixed-Time Controller:")
    print(f"  Mean Reward: {fixed_results['mean_reward']:.2f} Â± {fixed_results['std_reward']:.2f}")
    print(f"  Avg Queue Length: {fixed_results['mean_queue_length']:.2f}")
    print(f"  Avg Waiting Time: {fixed_results['mean_waiting_time']:.2f}")
    print(f"  Throughput: {fixed_results['mean_throughput']:.0f}")

    # Evaluate Max Pressure Controller
    max_pressure_controller = MaxPressureController()
    max_pressure_results = evaluate_controller(
        max_pressure_controller, eval_env, num_episodes=20
    )
    print(f"\nMax Pressure Controller:")
    print(f"  Mean Reward: {max_pressure_results['mean_reward']:.2f} Â± {max_pressure_results['std_reward']:.2f}")
    print(f"  Avg Queue Length: {max_pressure_results['mean_queue_length']:.2f}")
    print(f"  Avg Waiting Time: {max_pressure_results['mean_waiting_time']:.2f}")
    print(f"  Throughput: {max_pressure_results['mean_throughput']:.0f}")

    # Evaluate MLP Predictor
    mlp_predictor.eval()
    mlp_results = evaluate_controller(mlp_predictor, eval_env, num_episodes=20)
    print(f"\nMLP Predictor (Supervised):")
    print(f"  Mean Reward: {mlp_results['mean_reward']:.2f} Â± {mlp_results['std_reward']:.2f}")
    print(f"  Avg Queue Length: {mlp_results['mean_queue_length']:.2f}")
    print(f"  Avg Waiting Time: {mlp_results['mean_waiting_time']:.2f}")
    print(f"  Throughput: {mlp_results['mean_throughput']:.0f}")

    # Evaluate PPO Agent
    ppo_results = evaluate_controller(ppo_agent, eval_env, num_episodes=20, is_ppo=True)
    print(f"\nPPO Agent (Reinforcement Learning):")
    print(f"  Mean Reward: {ppo_results['mean_reward']:.2f} Â± {ppo_results['std_reward']:.2f}")
    print(f"  Avg Queue Length: {ppo_results['mean_queue_length']:.2f}")
    print(f"  Avg Waiting Time: {ppo_results['mean_waiting_time']:.2f}")
    print(f"  Throughput: {ppo_results['mean_throughput']:.0f}")
    return fixed_results, max_pressure_results, mlp_results, ppo_results


@app.cell(hide_code=True)
def visualization_header(mo):
    mo.md("""
    ## 9. Visualization and Analysis

    Generate comprehensive plots showing:
    - Training progress (rewards, losses)
    - Performance comparison across methods
    - Queue dynamics visualization
    """)
    return


@app.cell
def create_visualizations(
    fixed_results,
    max_pressure_results,
    mlp_losses,
    mlp_results,
    np,
    plt,
    ppo_results,
    training_results,
):
    # Create comprehensive visualization
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    # 1. Training Reward Curve
    ax1 = axes[0, 0]
    rewards = training_results["episode_rewards"]
    ax1.plot(rewards, alpha=0.3, color="blue", label="Episode Reward")
    # Moving average
    window = 10
    if len(rewards) >= window:
        moving_avg = np.convolve(rewards, np.ones(window) / window, mode="valid")
        ax1.plot(
            range(window - 1, len(rewards)),
            moving_avg,
            color="red",
            linewidth=2,
            label=f"{window}-Episode MA",
        )
    ax1.set_xlabel("Episode")
    ax1.set_ylabel("Total Reward")
    ax1.set_title("PPO Training: Episode Rewards")
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. Policy and Value Losses
    ax2 = axes[0, 1]
    if training_results["policy_losses"]:
        ax2.plot(
            training_results["policy_losses"], label="Policy Loss", color="purple"
        )
        ax2.plot(training_results["value_losses"], label="Value Loss", color="orange")
        ax2.set_xlabel("Update Step")
        ax2.set_ylabel("Loss")
        ax2.set_title("PPO Losses During Training")
        ax2.legend()
        ax2.grid(True, alpha=0.3)

    # 3. MLP Training Loss
    ax3 = axes[0, 2]
    ax3.plot(mlp_losses, color="green", linewidth=2)
    ax3.set_xlabel("Epoch")
    ax3.set_ylabel("Cross-Entropy Loss")
    ax3.set_title("MLP Baseline Training Loss")
    ax3.grid(True, alpha=0.3)

    # 4. Queue Length Comparison
    ax4 = axes[1, 0]
    methods = ["Fixed-Time", "Max Pressure", "MLP", "PPO"]
    queue_lengths = [
        fixed_results["mean_queue_length"],
        max_pressure_results["mean_queue_length"],
        mlp_results["mean_queue_length"],
        ppo_results["mean_queue_length"],
    ]
    colors = ["gray", "blue", "green", "red"]
    bars = ax4.bar(methods, queue_lengths, color=colors, alpha=0.7, edgecolor="black")
    ax4.set_ylabel("Average Queue Length")
    ax4.set_title("Queue Length Comparison")
    ax4.grid(True, alpha=0.3, axis="y")
    # Add value labels
    for bar, val in zip(bars, queue_lengths):
        ax4.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 0.1,
            f"{val:.2f}",
            ha="center",
            va="bottom",
            fontsize=10,
        )

    # 5. Reward Comparison
    ax5 = axes[1, 1]
    rewards_comparison = [
        fixed_results["mean_reward"],
        max_pressure_results["mean_reward"],
        mlp_results["mean_reward"],
        ppo_results["mean_reward"],
    ]
    reward_stds = [
        fixed_results["std_reward"],
        max_pressure_results["std_reward"],
        mlp_results["std_reward"],
        ppo_results["std_reward"],
    ]
    bars = ax5.bar(
        methods,
        rewards_comparison,
        yerr=reward_stds,
        color=colors,
        alpha=0.7,
        edgecolor="black",
        capsize=5,
    )
    ax5.set_ylabel("Mean Episode Reward")
    ax5.set_title("Reward Comparison (Higher is Better)")
    ax5.grid(True, alpha=0.3, axis="y")
    ax5.axhline(y=0, color="black", linestyle="--", alpha=0.5)

    # 6. Throughput Comparison
    ax6 = axes[1, 2]
    throughputs = [
        fixed_results["mean_throughput"],
        max_pressure_results["mean_throughput"],
        mlp_results["mean_throughput"],
        ppo_results["mean_throughput"],
    ]
    bars = ax6.bar(methods, throughputs, color=colors, alpha=0.7, edgecolor="black")
    ax6.set_ylabel("Average Throughput (vehicles)")
    ax6.set_title("Throughput Comparison (Higher is Better)")
    ax6.grid(True, alpha=0.3, axis="y")
    for bar, val in zip(bars, throughputs):
        ax6.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 5,
            f"{val:.0f}",
            ha="center",
            va="bottom",
            fontsize=10,
        )

    plt.tight_layout()
    plt.suptitle(
        "Smart Traffic Signal Control: PPO vs Baselines",
        fontsize=14,
        fontweight="bold",
        y=1.02,
    )

    fig
    return


@app.cell
def training_progress_detail(np, plt, training_results):
    # Detailed training progress
    fig2, axes2 = plt.subplots(1, 2, figsize=(14, 5))

    # Queue length over training
    ax_q = axes2[0]
    queue_data = training_results["avg_queue_lengths"]
    ax_q.plot(queue_data, alpha=0.5, color="blue")
    if len(queue_data) >= 10:
        ma = np.convolve(queue_data, np.ones(10) / 10, mode="valid")
        ax_q.plot(range(9, len(queue_data)), ma, color="red", linewidth=2, label="10-ep MA")
    ax_q.set_xlabel("Episode")
    ax_q.set_ylabel("Average Queue Length")
    ax_q.set_title("Queue Length During PPO Training")
    ax_q.legend()
    ax_q.grid(True, alpha=0.3)

    # Waiting time over training
    ax_w = axes2[1]
    wait_data = training_results["avg_waiting_times"]
    ax_w.plot(wait_data, alpha=0.5, color="orange")
    if len(wait_data) >= 10:
        ma_w = np.convolve(wait_data, np.ones(10) / 10, mode="valid")
        ax_w.plot(range(9, len(wait_data)), ma_w, color="red", linewidth=2, label="10-ep MA")
    ax_w.set_xlabel("Episode")
    ax_w.set_ylabel("Average Waiting Time")
    ax_w.set_title("Waiting Time During PPO Training")
    ax_w.legend()
    ax_w.grid(True, alpha=0.3)

    plt.tight_layout()
    fig2
    return


@app.cell(hide_code=True)
def summary_header(mo):
    mo.md("""
    ## 10. Results Summary and Conclusions
    """)
    return


@app.cell
def results_summary(
    fixed_results,
    max_pressure_results,
    mlp_results,
    mo,
    ppo_results,
):
    # Calculate improvements
    ppo_vs_fixed_reward = (
        (ppo_results["mean_reward"] - fixed_results["mean_reward"])
        / abs(fixed_results["mean_reward"])
        * 100
    )
    ppo_vs_fixed_queue = (
        (fixed_results["mean_queue_length"] - ppo_results["mean_queue_length"])
        / fixed_results["mean_queue_length"]
        * 100
    )
    ppo_vs_fixed_throughput = (
        (ppo_results["mean_throughput"] - fixed_results["mean_throughput"])
        / fixed_results["mean_throughput"]
        * 100
    )

    summary_table = f"""
    ## Performance Summary

    | Controller | Mean Reward | Avg Queue | Avg Wait Time | Throughput |
    |------------|-------------|-----------|---------------|------------|
    | Fixed-Time | {fixed_results['mean_reward']:.2f} Â± {fixed_results['std_reward']:.2f} | {fixed_results['mean_queue_length']:.2f} | {fixed_results['mean_waiting_time']:.2f} | {fixed_results['mean_throughput']:.0f} |
    | Max Pressure | {max_pressure_results['mean_reward']:.2f} Â± {max_pressure_results['std_reward']:.2f} | {max_pressure_results['mean_queue_length']:.2f} | {max_pressure_results['mean_waiting_time']:.2f} | {max_pressure_results['mean_throughput']:.0f} |
    | MLP (Supervised) | {mlp_results['mean_reward']:.2f} Â± {mlp_results['std_reward']:.2f} | {mlp_results['mean_queue_length']:.2f} | {mlp_results['mean_waiting_time']:.2f} | {mlp_results['mean_throughput']:.0f} |
    | **PPO (RL)** | **{ppo_results['mean_reward']:.2f}** Â± {ppo_results['std_reward']:.2f} | **{ppo_results['mean_queue_length']:.2f}** | **{ppo_results['mean_waiting_time']:.2f}** | **{ppo_results['mean_throughput']:.0f}** |

    ---

    ## Key Findings

    ### PPO vs Fixed-Time Controller:
    - **Reward Improvement**: {ppo_vs_fixed_reward:+.1f}%
    - **Queue Reduction**: {ppo_vs_fixed_queue:+.1f}%
    - **Throughput Change**: {ppo_vs_fixed_throughput:+.1f}%

    ### Analysis:

    1. **PPO Agent Performance**: The PPO-based reinforcement learning agent demonstrates {'superior' if ppo_results['mean_reward'] > fixed_results['mean_reward'] else 'competitive'} performance compared to traditional fixed-time control.

    2. **Adaptive Behavior**: Unlike fixed-time controllers, PPO learns to adapt signal timing based on real-time traffic conditions, leading to more efficient queue management.

    3. **MLP Baseline**: The supervised MLP provides a reasonable baseline but lacks the adaptive capabilities of RL, as it can only imitate historical behavior.

    4. **Max Pressure Controller**: Serves as a strong adaptive baseline, demonstrating the value of responsive signal control.

    ---

    ## Conclusions

    This implementation demonstrates the effectiveness of **Proximal Policy Optimization (PPO)** for adaptive traffic signal control:

    âœ… Successfully implemented Actor-Critic architecture for traffic control  
    âœ… PPO shows stable training with the clipped surrogate objective  
    âœ… The agent learns to minimize vehicle waiting times  
    âœ… Outperforms traditional fixed-time control approaches  

    ### Future Work:
    - Integration with SUMO simulator for more realistic scenarios
    - Multi-intersection coordination using multi-agent RL
    - Real-time deployment testing on Kathmandu traffic data
    """

    mo.md(summary_table)
    return


@app.cell(hide_code=True)
def appendix_header(mo):
    mo.md("""
    ---
    ## Appendix: Model Architecture Details

    ### A. PPO Actor-Critic Network
    ```
    Input Layer: 12 neurons (state dimension)
    â”œâ”€â”€ Shared Hidden: 256 neurons (Tanh activation)
    â”œâ”€â”€ Shared Hidden: 256 neurons (Tanh activation)
    â”‚
    â”œâ”€â”€ Actor Head: 4 neurons (action probabilities)
    â””â”€â”€ Critic Head: 1 neuron (state value)
    ```

    ### B. MLP Baseline Network
    ```
    Input Layer: 12 neurons
    â”œâ”€â”€ Hidden: 128 neurons (ReLU + BatchNorm + Dropout)
    â”œâ”€â”€ Hidden: 64 neurons (ReLU)
    â””â”€â”€ Output: 4 neurons (action logits)
    ```

    ### C. State Space Description
    | Feature | Dimension | Description |
    |---------|-----------|-------------|
    | Queue Lengths | 4 | Normalized vehicle counts per lane |
    | Waiting Times | 4 | Normalized cumulative wait times |
    | Current Phase | 4 | One-hot encoded signal phase |

    ### D. Action Space
    | Action | Description |
    |--------|-------------|
    | 0 | North-South through traffic green |
    | 1 | East-West through traffic green |
    | 2 | North-South left turn green |
    | 3 | East-West left turn green |
    """)
    return


if __name__ == "__main__":
    app.run()
